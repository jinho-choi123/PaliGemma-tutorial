{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"5c07048a-f5ce-45b8-ac7c-635317e48800","cell_type":"markdown","source":"## Prepare Dataset","metadata":{}},{"id":"4db9400e-2549-4647-b23b-ec4e576bbc37","cell_type":"code","source":"! pip install -q accelerate peft lightning nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T03:11:47.636557Z","iopub.execute_input":"2024-12-20T03:11:47.637442Z","iopub.status.idle":"2024-12-20T03:11:55.986400Z","shell.execute_reply.started":"2024-12-20T03:11:47.637391Z","shell.execute_reply":"2024-12-20T03:11:55.985496Z"}},"outputs":[],"execution_count":3},{"id":"c11480f8-230a-46be-8849-ffe59703512e","cell_type":"code","source":"! pip install -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T03:11:55.988058Z","iopub.execute_input":"2024-12-20T03:11:55.988335Z","iopub.status.idle":"2024-12-20T03:12:04.495496Z","shell.execute_reply.started":"2024-12-20T03:11:55.988308Z","shell.execute_reply":"2024-12-20T03:12:04.494381Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.45.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: typing_extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n","output_type":"stream"}],"execution_count":4},{"id":"bc1cd899-8b35-482d-b1a8-de4d76936771","cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T03:12:09.170457Z","iopub.execute_input":"2024-12-20T03:12:09.171312Z","iopub.status.idle":"2024-12-20T03:12:09.431352Z","shell.execute_reply.started":"2024-12-20T03:12:09.171275Z","shell.execute_reply":"2024-12-20T03:12:09.430513Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e850682437f648c9970a8b60868cc4c0"}},"metadata":{}}],"execution_count":5},{"id":"26d4efa6-bbc6-4c86-85b5-e1e0c9450036","cell_type":"code","source":"# download dataset\n\nimport transformers\n\nfrom datasets import load_dataset\n\nDATA_REPO_ID = \"tanganke/stanford_cars\"\n\n\n\ntrain_dataset = load_dataset(DATA_REPO_ID, split=\"train\")\n\ntest_dataset  = load_dataset(DATA_REPO_ID, split=\"test\")\n# valid_dataset = load_dataset(DATA_REPO_ID, split=\"validation\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T03:12:24.083895Z","iopub.execute_input":"2024-12-20T03:12:24.084315Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/11.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83ebab3d393d4a1e9cd033fe4543c604"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00002.parquet:   0%|          | 0.00/504M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0826463cc3204bf2941421475c3e006b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00002.parquet:   0%|          | 0.00/485M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a27d87196ee48b18efd03fd65baac46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00002.parquet:   0%|          | 0.00/513M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"863289304f3f43eda784d0bbb4ad57c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00001-of-00002.parquet:   0%|          | 0.00/474M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b82071b4c9c4deab7ccba8821c4dced"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"contrast-00000-of-00001.parquet:   0%|          | 0.00/347M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94e84286fe3c49da93ba26ff22bdb00a"}},"metadata":{}}],"execution_count":null},{"id":"fda4c198-31d8-460c-a60b-ad9adc50ac5b","cell_type":"code","source":"# prepare collate function\n\nfrom transformers import AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"google/paligemma-3b-pt-224\")\n\ndef train_collate_fn(samples):\n\n    images = [sample[\"image\"] for sample in samples]\n\n    texts = [\"What is the model of a car in the image?\" for sample in samples]\n\n    labels = [sample[\"label\"] for sample in samples]\n\n\n\n    # define a processor that handle max_length 128(not including the number of image tokens)\n\n    inputs = processor(text=texts, images=images, suffix=labels, return_tensors=\"pt\", \n\n                      padding=True, truncation=True, max_length=128)\n\n\n\n    input_ids = inputs[\"input_ids\"]\n\n    token_type_ids = inputs[\"token_type_ids\"]\n\n    attention_mask = inputs[\"attention_mask\"]\n\n    pixel_values = inputs[\"pixel_values\"]\n\n    labels = inputs[\"labels\"]\n\n\n\n    return input_ids, token_type_ids, attention_mask, pixel_values, labels\n\n\n\ndef test_collate_fn(samples):\n\n    images = [sample[\"image\"] for sample in samples]\n\n    texts = [\"What is the model of a car in the image?\" for sample in samples]\n\n    labels = [sample[\"label\"] for sample in samples]\n\n\n\n    # define a processor that handle max_length 128(not including the number of image tokens)\n\n    inputs = processor(text=texts, images=images, return_tensors=\"pt\", \n\n                      padding=True, truncation=True, max_length=128)\n\n\n\n    input_ids = inputs[\"input_ids\"]\n\n    attention_mask = inputs[\"attention_mask\"]\n\n    pixel_values = inputs[\"pixel_values\"]\n\n\n\n    return input_ids, attention_mask, pixel_values, labels\n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:57:00.517953Z","iopub.execute_input":"2024-12-19T13:57:00.518305Z","iopub.status.idle":"2024-12-19T13:57:18.287625Z","shell.execute_reply.started":"2024-12-19T13:57:00.518276Z","shell.execute_reply":"2024-12-19T13:57:18.286802Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/699 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef011dab083d408c9649d420282815e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/40.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5fd2edf40e94a8abc1c3b868b5ffaaf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.26M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63929bc33268452a8aa0755ea638c1d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea94c4fb2a914d3cb910749246bc7a85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86dd5a31685d443a919fbc4f0bc15abd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/607 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb2428d37adf4334b7993b2a77d17b6b"}},"metadata":{}}],"execution_count":4},{"id":"56718462-c8c3-4b41-ad36-b46b1ece960f","cell_type":"markdown","source":"## Prepare Quantization Settings","metadata":{}},{"id":"67c48c00-48e9-44a3-9746-ff7beaac1784","cell_type":"code","source":"# Using bitsandbytes library, setting the quantization\n\nfrom transformers import BitsAndBytesConfig\n\nimport torch\n\n\n\nbnb_config = BitsAndBytesConfig(\n\n    load_in_4bit=True,\n\n    bnb_4bit_compute_dtype=torch.bfloat16, # convert float32 to bf16 to speed up computation\n\n    bnb_4bit_quant_type=\"nf4\", # NF4 is 4-bit data type from QLoRA paper\n\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T14:02:21.457265Z","iopub.execute_input":"2024-12-19T14:02:21.457604Z","iopub.status.idle":"2024-12-19T14:02:21.464302Z","shell.execute_reply.started":"2024-12-19T14:02:21.457577Z","shell.execute_reply":"2024-12-19T14:02:21.463359Z"}},"outputs":[],"execution_count":17},{"id":"be6be565-111a-4369-86b2-1815ba745813","cell_type":"markdown","source":"## Prepare LoRA Settings","metadata":{}},{"id":"a24f35ef-9844-467e-a053-a91792936716","cell_type":"code","source":"from peft import LoraConfig\n\n\n\nlora_config = LoraConfig(\n\n    r=4, # set the low-rank as 4 \n\n    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], #modules that we are going to apply LoRA adapter\n\n    task_type=\"CAUSAL_LM\", # type of model. PaliGemma is causal language model\n\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:59:19.128337Z","iopub.execute_input":"2024-12-19T13:59:19.129075Z","iopub.status.idle":"2024-12-19T13:59:19.439081Z","shell.execute_reply.started":"2024-12-19T13:59:19.129040Z","shell.execute_reply":"2024-12-19T13:59:19.438305Z"}},"outputs":[],"execution_count":8},{"id":"045361e1-f555-4248-99a2-12735649a745","cell_type":"markdown","source":"## Prepare pytorch-lightning Trainer","metadata":{}},{"id":"96907c95-4e78-45d4-a66c-d3bead4fea84","cell_type":"code","source":"import lightning as L\n\nfrom transformers import AutoProcessor\n\nimport torch \n\nfrom nltk import edit_distance\n\nfrom torch.utils.data import DataLoader\n\n\n\n\n\nclass PaliGemma_Finetuned_Model(L.LightningModule):\n\n    def __init__(self, config, model, processor):\n\n        self.model = model \n\n        self.processor = processor\n\n        self.config = config\n\n\n\n        self.batch_size = config.get(\"batch_size\")\n\n\n\n        self.train_losses = []\n\n        self.val_losses = []\n\n        self.val_scores = []\n\n\n\n    def training_step(self, batch, batch_idx):\n\n        input_ids, token_type_ids, attention_mask, pixel_values, labels = batch\n\n\n\n        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids,\n\n                            pixel_values=pixel_values, labels=labels)\n\n\n\n        loss = outputs.loss\n\n\n\n        self.train_losses.append(loss.item())\n\n\n\n        self.log(\"train_loss\", loss)\n\n        return loss\n\n\n\n    def validation_step(self, batch, batch_idx):\n\n        input_ids, attention_mask, pixel_values, labels = batch\n\n\n\n        generated_ids = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values, max_new_tokens=80)\n\n        scores = []\n\n        \n\n        for pred, label in zip(predictions, labels):\n\n            score = edit_distance(pred, label) / max(len(pred), len(label))\n\n            self.val_scores.append(score)\n\n            scores.append(score)\n\n        self.log(\"val_edit_distance\", np.mean(scores), on_epoch=True)\n\n\n\n        return scores\n\n\n\n    def configure_optimizers(self):\n\n        optimizer = torch.optim.AdamW(self.parameters(), lr=config.get(\"lr\", 3e-4))\n\n        return optimizer\n\n\n\n    def train_dataloader(self):\n\n        return DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=train_collate_fn, num_workers=3)\n\n    def val_dataloader(self):\n\n        return DataLoader(test_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=test_collate_fn, num_workers=3)\n\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T14:01:13.997785Z","iopub.execute_input":"2024-12-19T14:01:13.998139Z","iopub.status.idle":"2024-12-19T14:01:14.010592Z","shell.execute_reply.started":"2024-12-19T14:01:13.998109Z","shell.execute_reply":"2024-12-19T14:01:14.009541Z"}},"outputs":[],"execution_count":15},{"id":"5a733a06-ed00-420a-8c48-8e60a4c4976d","cell_type":"markdown","source":"## Define the model ","metadata":{}},{"id":"9e131d3d-04bd-4f1e-b8ca-02a1481a8e6d","cell_type":"code","source":"from transformers import PaliGemmaForConditionalGeneration\n\nfrom peft import get_peft_model\n\n\n\nmodel = PaliGemmaForConditionalGeneration.from_pretrained(\"google/paligemma-3b-pt-224\", quantization_config=bnb_config)\n\nmodel = get_peft_model(model, lora_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T14:02:29.836347Z","iopub.execute_input":"2024-12-19T14:02:29.836712Z","iopub.status.idle":"2024-12-19T14:02:30.156612Z","shell.execute_reply.started":"2024-12-19T14:02:29.836682Z","shell.execute_reply":"2024-12-19T14:02:30.155394Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PaliGemmaForConditionalGeneration\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_peft_model\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPaliGemmaForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle/paligemma-3b-pt-224\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m get_peft_model(model, lora_config)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3657\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3654\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3657\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3659\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3660\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3661\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:74\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 4-bit quantization requires Accelerate: `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m     )\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_bitsandbytes_available():\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_bnb_backend_availability\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n","\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`"],"ename":"ImportError","evalue":"Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`","output_type":"error"}],"execution_count":18},{"id":"7654cd6e-b62d-4273-b03d-60c61c0d7e40","cell_type":"code","source":"model.print_trainable_parameters()","metadata":{},"outputs":[],"execution_count":null},{"id":"bb1f970e-503c-4571-bfb4-5cde62567fc9","cell_type":"markdown","source":"## Setup Training Configurations","metadata":{}},{"id":"d7debc62-d578-410d-b205-e1fb6b657b7b","cell_type":"code","source":"config = config = {\"max_epochs\": EPOCHS,\n\n          \"check_val_every_n_epoch\": 1,\n\n          \"gradient_clip_val\": 1.0,\n\n          \"accumulate_grad_batches\": 8,\n\n          \"lr\": 3e-4,\n\n          \"batch_size\": 2,\n\n          \"seed\":1234,\n\n          \"num_nodes\": 1,\n\n          \"warmup_steps\": 50,\n\n          \"result_path\": \"./result\",\n\n          \"verbose\": True,\n\n}\n\n\n\nmodel_module = PaliGemma_Finetuned_Model(config, processor, model)","metadata":{},"outputs":[],"execution_count":null},{"id":"941ae29e-765d-4500-b384-39c8e332572a","cell_type":"markdown","source":"## Define Callbacks","metadata":{}},{"id":"59037b17-be93-45c6-8592-1d37581bb2e4","cell_type":"code","source":"from lightning.pytorch.callbacks import Callback\n\nfrom lightning.pytorch.callbacks.early_stopping import EarlyStopping\n\nfrom huggingface_hub import HfApi\n\n\n\napi = HfApi()\n\n\n\nFINETUNED_MODEL_ID=\"ball1433/PaliGemma-StanfordCars-finetuned\"\n\n\n\nclass Print_TrainValidation_ResultCallback(Callback):\n\n    def on_train_epoch_end(self, trainer, pl_module):\n\n        # print the average of training loss \n\n        print(f'Average Training Loss: {np.mean(pl_module.train_losses)}')\n\n\n\n        # print the average of edit distance score\n\n        print(f'Average Validation Score: {np.mean(pl_module.val_scores)}')\n\n\n\n        # reset the list\n\n        pl_module.train_losses = []\n\n        pl_module.val_scores = []\n\n\n\n\n\nclass PushToHubCallback(Callback):\n\n    def on_train_epoch_end(self, trainer, pl_module):\n\n        print(f\"Pushing model to the hub, epoch {trainer.current_epoch}\")\n\n        pl_module.model.push_to_hub(FINETUNED_MODEL_ID,\n\n                                    commit_message=f\"Training in progress, epoch {trainer.current_epoch}\")\n\n\n\n    def on_train_end(self, trainer, pl_module):\n\n        print(f\"Pushing model to the hub after training\")\n\n        pl_module.processor.push_to_hub(FINETUNED_MODEL_ID,\n\n                                    commit_message=f\"Training done\")\n\n        pl_module.model.push_to_hub(FINETUNED_MODEL_ID,\n\n                                    commit_message=f\"Training done\")\n\n\n\nearly_stop_callback = EarlyStopping(monitor=\"val_edit_distance\", patience=20, verbose=False, mode=\"min\")","metadata":{},"outputs":[],"execution_count":null},{"id":"95a43f57-6379-4bf5-95b8-745eb9ec1737","cell_type":"markdown","source":"## Training","metadata":{}},{"id":"c5b43acc-eecd-483c-bb86-607deab32d7f","cell_type":"code","source":"# define trainer \n\ntrainer = L.Trainer(\n\n        devices=-1, \n\n        accelerator=\"auto\",\n\n        max_epochs=config.get(\"max_epochs\"),\n\n        accumulate_grad_batches=config.get(\"accumulate_grad_batches\"),\n\n        check_val_every_n_epoch=config.get(\"check_val_every_n_epoch\"),\n\n        gradient_clip_val=config.get(\"gradient_clip_val\"),\n\n        precision=\"16-mixed\",\n\n        limit_val_batches=1.0,\n\n        num_sanity_val_steps=2,\n\n        callbacks=[PushToHubCallback(), Print_TrainValidation_ResultCallback(), early_stop_callback],\n\n)\n\n\n\ntrainer.fit(model_module)","metadata":{},"outputs":[],"execution_count":null},{"id":"ac92c16d-1ace-4e15-94f4-67c70a50d3a5","cell_type":"markdown","source":"## Inference","metadata":{}}]}