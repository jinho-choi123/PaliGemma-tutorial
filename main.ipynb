{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"5c07048a-f5ce-45b8-ac7c-635317e48800","cell_type":"markdown","source":"## Prepare Dataset","metadata":{}},{"id":"4db9400e-2549-4647-b23b-ec4e576bbc37","cell_type":"code","source":"! pip install -q accelerate peft lightning nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T05:33:52.382099Z","iopub.execute_input":"2024-12-20T05:33:52.382430Z","iopub.status.idle":"2024-12-20T05:34:03.034158Z","shell.execute_reply.started":"2024-12-20T05:33:52.382401Z","shell.execute_reply":"2024-12-20T05:34:03.032962Z"}},"outputs":[],"execution_count":2},{"id":"c11480f8-230a-46be-8849-ffe59703512e","cell_type":"code","source":"! pip install -q -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T05:34:03.036443Z","iopub.execute_input":"2024-12-20T05:34:03.037234Z","iopub.status.idle":"2024-12-20T05:34:13.637086Z","shell.execute_reply.started":"2024-12-20T05:34:03.037191Z","shell.execute_reply":"2024-12-20T05:34:13.635928Z"}},"outputs":[],"execution_count":3},{"id":"bc1cd899-8b35-482d-b1a8-de4d76936771","cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T05:33:46.759715Z","iopub.execute_input":"2024-12-20T05:33:46.760538Z","iopub.status.idle":"2024-12-20T05:33:47.065442Z","shell.execute_reply.started":"2024-12-20T05:33:46.760478Z","shell.execute_reply":"2024-12-20T05:33:47.064425Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25d83eaca9284c2a92d0caffccc838a8"}},"metadata":{}}],"execution_count":1},{"id":"26d4efa6-bbc6-4c86-85b5-e1e0c9450036","cell_type":"code","source":"# download dataset\n\nimport transformers\nimport datasets\nfrom pathlib import Path\nimport os\n\nDATA_REPO_ID = \"tanganke/stanford_cars\"\n\nclass_names = [\n    'AM General Hummer SUV 2000',\n    'Acura RL Sedan 2012',\n    'Acura TL Sedan 2012',\n    'Acura TL Type-S 2008',\n    'Acura TSX Sedan 2012',\n    'Acura Integra Type R 2001',\n    'Acura ZDX Hatchback 2012',\n    'Aston Martin V8 Vantage Convertible 2012',\n    'Aston Martin V8 Vantage Coupe 2012',\n    'Aston Martin Virage Convertible 2012',\n    'Aston Martin Virage Coupe 2012',\n    'Audi RS 4 Convertible 2008',\n    'Audi A5 Coupe 2012',\n    'Audi TTS Coupe 2012',\n    'Audi R8 Coupe 2012',\n    'Audi V8 Sedan 1994',\n    'Audi 100 Sedan 1994',\n    'Audi 100 Wagon 1994',\n    'Audi TT Hatchback 2011',\n    'Audi S6 Sedan 2011',\n    'Audi S5 Convertible 2012',\n    'Audi S5 Coupe 2012',\n    'Audi S4 Sedan 2012',\n    'Audi S4 Sedan 2007',\n    'Audi TT RS Coupe 2012',\n    'BMW ActiveHybrid 5 Sedan 2012',\n    'BMW 1 Series Convertible 2012',\n    'BMW 1 Series Coupe 2012',\n    'BMW 3 Series Sedan 2012',\n    'BMW 3 Series Wagon 2012',\n    'BMW 6 Series Convertible 2007',\n    'BMW X5 SUV 2007',\n    'BMW X6 SUV 2012',\n    'BMW M3 Coupe 2012',\n    'BMW M5 Sedan 2010',\n    'BMW M6 Convertible 2010',\n    'BMW X3 SUV 2012',\n    'BMW Z4 Convertible 2012',\n    'Bentley Continental Supersports Conv. Convertible 2012',\n    'Bentley Arnage Sedan 2009',\n    'Bentley Mulsanne Sedan 2011',\n    'Bentley Continental GT Coupe 2012',\n    'Bentley Continental GT Coupe 2007',\n    'Bentley Continental Flying Spur Sedan 2007',\n    'Bugatti Veyron 16.4 Convertible 2009',\n    'Bugatti Veyron 16.4 Coupe 2009',\n    'Buick Regal GS 2012',\n    'Buick Rainier SUV 2007',\n    'Buick Verano Sedan 2012',\n    'Buick Enclave SUV 2012',\n    'Cadillac CTS-V Sedan 2012',\n    'Cadillac SRX SUV 2012',\n    'Cadillac Escalade EXT Crew Cab 2007',\n    'Chevrolet Silverado 1500 Hybrid Crew Cab 2012',\n    'Chevrolet Corvette Convertible 2012',\n    'Chevrolet Corvette ZR1 2012',\n    'Chevrolet Corvette Ron Fellows Edition Z06 2007',\n    'Chevrolet Traverse SUV 2012',\n    'Chevrolet Camaro Convertible 2012',\n    'Chevrolet HHR SS 2010',\n    'Chevrolet Impala Sedan 2007',\n    'Chevrolet Tahoe Hybrid SUV 2012',\n    'Chevrolet Sonic Sedan 2012',\n    'Chevrolet Express Cargo Van 2007',\n    'Chevrolet Avalanche Crew Cab 2012',\n    'Chevrolet Cobalt SS 2010',\n    'Chevrolet Malibu Hybrid Sedan 2010',\n    'Chevrolet TrailBlazer SS 2009',\n    'Chevrolet Silverado 2500HD Regular Cab 2012',\n    'Chevrolet Silverado 1500 Classic Extended Cab 2007',\n    'Chevrolet Express Van 2007',\n    'Chevrolet Monte Carlo Coupe 2007',\n    'Chevrolet Malibu Sedan 2007',\n    'Chevrolet Silverado 1500 Extended Cab 2012',\n    'Chevrolet Silverado 1500 Regular Cab 2012',\n    'Chrysler Aspen SUV 2009',\n    'Chrysler Sebring Convertible 2010',\n    'Chrysler Town and Country Minivan 2012',\n    'Chrysler 300 SRT-8 2010',\n    'Chrysler Crossfire Convertible 2008',\n    'Chrysler PT Cruiser Convertible 2008',\n    'Daewoo Nubira Wagon 2002',\n    'Dodge Caliber Wagon 2012',\n    'Dodge Caliber Wagon 2007',\n    'Dodge Caravan Minivan 1997',\n    'Dodge Ram Pickup 3500 Crew Cab 2010',\n    'Dodge Ram Pickup 3500 Quad Cab 2009',\n    'Dodge Sprinter Cargo Van 2009',\n    'Dodge Journey SUV 2012',\n    'Dodge Dakota Crew Cab 2010',\n    'Dodge Dakota Club Cab 2007',\n    'Dodge Magnum Wagon 2008',\n    'Dodge Challenger SRT8 2011',\n    'Dodge Durango SUV 2012',\n    'Dodge Durango SUV 2007',\n    'Dodge Charger Sedan 2012',\n    'Dodge Charger SRT-8 2009',\n    'Eagle Talon Hatchback 1998',\n    'FIAT 500 Abarth 2012',\n    'FIAT 500 Convertible 2012',\n    'Ferrari FF Coupe 2012',\n    'Ferrari California Convertible 2012',\n    'Ferrari 458 Italia Convertible 2012',\n    'Ferrari 458 Italia Coupe 2012',\n    'Fisker Karma Sedan 2012',\n    'Ford F-450 Super Duty Crew Cab 2012',\n    'Ford Mustang Convertible 2007',\n    'Ford Freestar Minivan 2007',\n    'Ford Expedition EL SUV 2009',\n    'Ford Edge SUV 2012',\n    'Ford Ranger SuperCab 2011',\n    'Ford GT Coupe 2006',\n    'Ford F-150 Regular Cab 2012',\n    'Ford F-150 Regular Cab 2007',\n    'Ford Focus Sedan 2007',\n    'Ford E-Series Wagon Van 2012',\n    'Ford Fiesta Sedan 2012',\n    'GMC Terrain SUV 2012',\n    'GMC Savana Van 2012',\n    'GMC Yukon Hybrid SUV 2012',\n    'GMC Acadia SUV 2012',\n    'GMC Canyon Extended Cab 2012',\n    'Geo Metro Convertible 1993',\n    'HUMMER H3T Crew Cab 2010',\n    'HUMMER H2 SUT Crew Cab 2009',\n    'Honda Odyssey Minivan 2012',\n    'Honda Odyssey Minivan 2007',\n    'Honda Accord Coupe 2012',\n    'Honda Accord Sedan 2012',\n    'Hyundai Veloster Hatchback 2012',\n    'Hyundai Santa Fe SUV 2012',\n    'Hyundai Tucson SUV 2012',\n    'Hyundai Veracruz SUV 2012',\n    'Hyundai Sonata Hybrid Sedan 2012',\n    'Hyundai Elantra Sedan 2007',\n    'Hyundai Accent Sedan 2012',\n    'Hyundai Genesis Sedan 2012',\n    'Hyundai Sonata Sedan 2012',\n    'Hyundai Elantra Touring Hatchback 2012',\n    'Hyundai Azera Sedan 2012',\n    'Infiniti G Coupe IPL 2012',\n    'Infiniti QX56 SUV 2011',\n    'Isuzu Ascender SUV 2008',\n    'Jaguar XK XKR 2012',\n    'Jeep Patriot SUV 2012',\n    'Jeep Wrangler SUV 2012',\n    'Jeep Liberty SUV 2012',\n    'Jeep Grand Cherokee SUV 2012',\n    'Jeep Compass SUV 2012',\n    'Lamborghini Reventon Coupe 2008',\n    'Lamborghini Aventador Coupe 2012',\n    'Lamborghini Gallardo LP 570-4 Superleggera 2012',\n    'Lamborghini Diablo Coupe 2001',\n    'Land Rover Range Rover SUV 2012',\n    'Land Rover LR2 SUV 2012',\n    'Lincoln Town Car Sedan 2011',\n    'MINI Cooper Roadster Convertible 2012',\n    'Maybach Landaulet Convertible 2012',\n    'Mazda Tribute SUV 2011',\n    'McLaren MP4-12C Coupe 2012',\n    'Mercedes-Benz 300-Class Convertible 1993',\n    'Mercedes-Benz C-Class Sedan 2012',\n    'Mercedes-Benz SL-Class Coupe 2009',\n    'Mercedes-Benz E-Class Sedan 2012',\n    'Mercedes-Benz S-Class Sedan 2012',\n    'Mercedes-Benz Sprinter Van 2012',\n    'Mitsubishi Lancer Sedan 2012',\n    'Nissan Leaf Hatchback 2012',\n    'Nissan NV Passenger Van 2012',\n    'Nissan Juke Hatchback 2012',\n    'Nissan 240SX Coupe 1998',\n    'Plymouth Neon Coupe 1999',\n    'Porsche Panamera Sedan 2012',\n    'Ram C/V Cargo Van Minivan 2012',\n    'Rolls-Royce Phantom Drophead Coupe Convertible 2012',\n    'Rolls-Royce Ghost Sedan 2012',\n    'Rolls-Royce Phantom Sedan 2012',\n    'Scion xD Hatchback 2012',\n    'Spyker C8 Convertible 2009',\n    'Spyker C8 Coupe 2009',\n    'Suzuki Aerio Sedan 2007',\n    'Suzuki Kizashi Sedan 2012',\n    'Suzuki SX4 Hatchback 2012',\n    'Suzuki SX4 Sedan 2012',\n    'Tesla Model S Sedan 2012',\n    'Toyota Sequoia SUV 2012',\n    'Toyota Camry Sedan 2012',\n    'Toyota Corolla Sedan 2012',\n    'Toyota 4Runner SUV 2012',\n    'Volkswagen Golf Hatchback 2012',\n    'Volkswagen Golf Hatchback 1991',\n    'Volkswagen Beetle Hatchback 2012',\n    'Volvo C30 Hatchback 2012',\n    'Volvo 240 Sedan 1993',\n    'Volvo XC90 SUV 2007',\n    'smart fortwo Convertible 2012',\n]\n\nconvertDict = dict()\nfor i in range(len(class_names)):\n    convertDict[i] = class_names[i]\n\ndef classid2name(classid):\n    return convertDict[classid]\n    \n\ntrain_dataset = datasets.load_dataset(DATA_REPO_ID, split=\"train\")\n\ntest_dataset  = datasets.load_dataset(DATA_REPO_ID, split=\"test\")\n# valid_dataset = load_dataset(DATA_REPO_ID, split=\"validation\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T05:34:56.730380Z","iopub.execute_input":"2024-12-20T05:34:56.730741Z","iopub.status.idle":"2024-12-20T05:37:52.503046Z","shell.execute_reply.started":"2024-12-20T05:34:56.730711Z","shell.execute_reply":"2024-12-20T05:37:52.502332Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/11.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bc61323a8f847a4a5d853d9eb186b50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00002.parquet:   0%|          | 0.00/504M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5634846cbed44be1ae4cbfb63df004f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00002.parquet:   0%|          | 0.00/485M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5df7bec1e62647a7ad636d51a773e4bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00002.parquet:   0%|          | 0.00/513M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10c711a1ee724996af597845e74ccc54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00001-of-00002.parquet:   0%|          | 0.00/474M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13d753227c96473c9547d2bc06c90743"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"contrast-00000-of-00001.parquet:   0%|          | 0.00/347M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6490e7341b9a483491d3c80357bef26f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"gaussian_noise-00000-of-00002.parquet:   0%|          | 0.00/475M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a05ae326ff14b8481996c3b68a173d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"gaussian_noise-00001-of-00002.parquet:   0%|          | 0.00/450M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41cff26cc7924529bbdbf4b7f10d34db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"impulse_noise-00000-of-00002.parquet:   0%|          | 0.00/543M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffe239a6bcfe462eae64d7ca6719b7e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"impulse_noise-00001-of-00002.parquet:   0%|          | 0.00/513M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"701c78cd2ab7499b8420316410505165"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"jpeg_compression-00000-of-00001.parquet:   0%|          | 0.00/467M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b40a0243243d45ad840b760f8abbdd92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"motion_blur-00000-of-00001.parquet:   0%|          | 0.00/435M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"884987c200a042c5aa0fc353d3c0e603"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pixelate-00000-of-00001.parquet:   0%|          | 0.00/3.74M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cb5ce08f76343389b4445aef99558bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spatter-00000-of-00002.parquet:   0%|          | 0.00/417M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17e1260214584efa8218956f53e6e689"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spatter-00001-of-00002.parquet:   0%|          | 0.00/391M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"330d4d1864c34a759fac51d232df201f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/8144 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4c256793b3f4f2f9f0a01b6ea961cd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/8041 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a366a0786a934e4e91bb6cbec97fa616"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating contrast split:   0%|          | 0/8041 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32d8f483758d45c0ad5083129bce1c95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating gaussian_noise split:   0%|          | 0/8041 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dcb575a02f04f20933e59c8420cdf64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating impulse_noise split:   0%|          | 0/8041 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"410474db924f44c1aca15b4673996391"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating jpeg_compression split:   0%|          | 0/8041 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a049fc868efc417c9cb211a075601643"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating motion_blur split:   0%|          | 0/8041 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61ca2676fe1c4329b2303cdc3819a88a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating pixelate split:   0%|          | 0/8041 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"676218a46f7249809f2e3d63d38452f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating spatter split:   0%|          | 0/8041 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e304fa1c1ab647b5a08e70af798b04e6"}},"metadata":{}}],"execution_count":4},{"id":"fda4c198-31d8-460c-a60b-ad9adc50ac5b","cell_type":"code","source":"# prepare collate function\n\nfrom transformers import AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"google/paligemma-3b-pt-224\")\n\ndef train_collate_fn(samples):\n\n    images = []\n\n    for image in [sample[\"image\"] for sample in samples]:\n        if image.mode != \"RGB\":  # Check if the image is not already RGB\n            images.append(image.convert(\"RGB\"))\n        else:\n            images.append(image)\n\n    texts = [\"<image><bos>\" + \"What is the model of a car in the image?\" for sample in samples]\n\n    labels = [classid2name(sample[\"label\"])+\"<eos>\" for sample in samples]\n\n\n\n    # define a processor that handle max_length 128(not including the number of image tokens)\n\n    inputs = processor(text=texts, images=images, suffix=labels, return_tensors=\"pt\", \n\n                      padding=True, truncation=True, max_length=128)\n\n\n\n    input_ids = inputs[\"input_ids\"]\n\n    token_type_ids = inputs[\"token_type_ids\"]\n\n    attention_mask = inputs[\"attention_mask\"]\n\n    pixel_values = inputs[\"pixel_values\"]\n\n    labels = inputs[\"labels\"]\n\n\n\n    return input_ids, token_type_ids, attention_mask, pixel_values, labels\n\n\n\ndef test_collate_fn(samples):\n\n    images = []\n\n    for image in [sample[\"image\"] for sample in samples]:\n        if image.mode != \"RGB\":  # Check if the image is not already RGB\n            images.append(image.convert(\"RGB\"))\n        else:\n            images.append(image)\n\n    texts = [\"<image><bos>\" + \"What is the model of a car in the image?\" for sample in samples]\n\n    labels = [classid2name(sample[\"label\"]) + \"<eos>\" for sample in samples]\n\n\n\n    # define a processor that handle max_length 128(not including the number of image tokens)\n\n    inputs = processor(text=texts, images=images, return_tensors=\"pt\", \n\n                      padding=True, truncation=True, max_length=128)\n\n\n\n    input_ids = inputs[\"input_ids\"]\n\n    attention_mask = inputs[\"attention_mask\"]\n\n    pixel_values = inputs[\"pixel_values\"]\n\n\n\n    return input_ids, attention_mask, pixel_values, labels\n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T05:37:52.504766Z","iopub.execute_input":"2024-12-20T05:37:52.505324Z","iopub.status.idle":"2024-12-20T05:38:10.730170Z","shell.execute_reply.started":"2024-12-20T05:37:52.505285Z","shell.execute_reply":"2024-12-20T05:38:10.729454Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/699 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5db5f2ba8f94c3381ec6a4a076d4ebf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/40.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56c49afd00334ba4b6b1cc40bdc3a4c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.26M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a38799ef4c094e13868571848706f38f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"204549c8a91e4175be46afa9eedb5c80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d70f3384ff3d49b395e4bea19c67a84a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/607 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5fa30f570df442d95d92b38e4961518"}},"metadata":{}}],"execution_count":5},{"id":"56a3d1f5-9160-426f-827b-3e67b34e6274","cell_type":"code","source":"# TMP TEST PURPOSE\n# tmp_train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=train_collate_fn, num_workers=3)\n\n# for i in range(len(train_dataset)):\n#     input_ids, token_type_ids, attention_mask, pixel_values, labels = next(iter(tmp_train_dataloader))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T05:38:10.731191Z","iopub.execute_input":"2024-12-20T05:38:10.731708Z","iopub.status.idle":"2024-12-20T05:38:10.735599Z","shell.execute_reply.started":"2024-12-20T05:38:10.731680Z","shell.execute_reply":"2024-12-20T05:38:10.734591Z"}},"outputs":[],"execution_count":6},{"id":"56718462-c8c3-4b41-ad36-b46b1ece960f","cell_type":"markdown","source":"## Prepare Quantization Settings","metadata":{}},{"id":"67c48c00-48e9-44a3-9746-ff7beaac1784","cell_type":"code","source":"# Using bitsandbytes library, setting the quantization\n\nfrom transformers import BitsAndBytesConfig\n\nimport torch\n\nbnb_config = BitsAndBytesConfig(\n\n    load_in_4bit=True,\n\n    bnb_4bit_compute_dtype=torch.bfloat16, # convert float32 to bf16 to speed up computation\n\n    bnb_4bit_quant_type=\"nf4\", # NF4 is 4-bit data type from QLoRA paper\n\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T05:38:10.737829Z","iopub.execute_input":"2024-12-20T05:38:10.738700Z","iopub.status.idle":"2024-12-20T05:38:10.822051Z","shell.execute_reply.started":"2024-12-20T05:38:10.738661Z","shell.execute_reply":"2024-12-20T05:38:10.821282Z"}},"outputs":[],"execution_count":7},{"id":"be6be565-111a-4369-86b2-1815ba745813","cell_type":"markdown","source":"## Prepare LoRA Settings","metadata":{}},{"id":"a24f35ef-9844-467e-a053-a91792936716","cell_type":"code","source":"from peft import LoraConfig\n\n\nlora_config = LoraConfig(\n\n    r=4, # set the low-rank as 4 \n\n    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], #modules that we are going to apply LoRA adapter\n\n    task_type=\"CAUSAL_LM\", # type of model. PaliGemma is causal language model\n\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T05:38:10.823122Z","iopub.execute_input":"2024-12-20T05:38:10.823431Z","iopub.status.idle":"2024-12-20T05:38:11.109263Z","shell.execute_reply.started":"2024-12-20T05:38:10.823406Z","shell.execute_reply":"2024-12-20T05:38:11.108389Z"}},"outputs":[],"execution_count":8},{"id":"045361e1-f555-4248-99a2-12735649a745","cell_type":"markdown","source":"## Prepare pytorch-lightning Trainer","metadata":{}},{"id":"96907c95-4e78-45d4-a66c-d3bead4fea84","cell_type":"code","source":"import lightning as L\n\nfrom transformers import AutoProcessor\n\nimport torch \n\nfrom nltk import edit_distance\n\nfrom torch.utils.data import DataLoader\n\nimport numpy as np\n\n\n\n\n\nclass PaliGemma_Finetuned_Model(L.LightningModule):\n\n    def __init__(self, config, model, processor):\n        super().__init__()\n\n        self.model = model \n\n        self.processor = processor\n\n        self.config = config\n\n\n\n        self.batch_size = config.get(\"batch_size\")\n\n\n\n        self.train_losses = []\n\n        self.val_losses = []\n\n        self.val_scores = []\n\n\n\n    def training_step(self, batch, batch_idx):\n        input_ids, token_type_ids, attention_mask, pixel_values, labels = batch\n\n\n\n        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids,\n\n                            pixel_values=pixel_values, labels=labels)\n\n\n\n        loss = outputs.loss\n\n        self.train_losses.append(loss.item())\n\n\n\n        self.log(\"train_loss\", loss)\n\n        return loss\n\n\n\n    def validation_step(self, batch, batch_idx):\n\n        input_ids, attention_mask, pixel_values, labels = batch\n\n\n\n        generated_ids = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values, max_new_tokens=80)\n        predictions = self.processor.batch_decode(generated_ids[:, input_ids.size(1)+1:], skip_special_tokens=True)\n    \n\n        scores = []\n\n        for pred, label in zip(predictions, labels):\n\n            score = edit_distance(pred, label) / max(len(pred), len(label))\n\n            self.val_scores.append(score)\n\n            scores.append(score)\n\n        self.log(\"val_edit_distance\", np.mean(scores), on_epoch=True)\n\n        if self.trainer.sanity_checking:\n            print(f\"SANITY CHECKING STEP\")\n            print(f\"batch_decoded input_ids: {self.processor.batch_decode(input_ids, skip_special_tokens=True)}\")\n            print(f\"labels: {labels}\")\n            print(f\"prediction: {predictions}\")\n            print(f\"edit_distance_score: {np.mean(scores)}\")\n\n        elif batch_idx % 2000 == 0:\n            print(f\"VALIDATION STEP\")\n            print(f\"batch_decoded input_ids: {self.processor.batch_decode(input_ids, skip_special_tokens=True)}\")\n            print(f\"labels: {labels}\")\n            print(f\"prediction: {predictions}\")\n            print(f\"edit_distance_score: {np.mean(scores)}\")\n            \n            \n\n\n\n        return scores\n\n\n\n    def configure_optimizers(self):\n\n        optimizer = torch.optim.AdamW(self.parameters(), lr=config.get(\"lr\", 3e-4))\n\n        return optimizer\n\n\n\n    def train_dataloader(self):\n\n        return DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=train_collate_fn, num_workers=3)\n\n    def val_dataloader(self):\n\n        return DataLoader(test_dataset, batch_size=self.batch_size, collate_fn=test_collate_fn, num_workers=3)\n\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T05:38:11.110434Z","iopub.execute_input":"2024-12-20T05:38:11.111056Z","iopub.status.idle":"2024-12-20T05:38:12.783255Z","shell.execute_reply.started":"2024-12-20T05:38:11.111027Z","shell.execute_reply":"2024-12-20T05:38:12.782496Z"}},"outputs":[],"execution_count":9},{"id":"5a733a06-ed00-420a-8c48-8e60a4c4976d","cell_type":"markdown","source":"## Define the model ","metadata":{}},{"id":"9e131d3d-04bd-4f1e-b8ca-02a1481a8e6d","cell_type":"code","source":"from transformers import PaliGemmaForConditionalGeneration\n\nfrom peft import get_peft_model\n\n\n\nmodel = PaliGemmaForConditionalGeneration.from_pretrained(\"google/paligemma-3b-pt-224\", quantization_config=bnb_config)\n\nmodel = get_peft_model(model, lora_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T05:38:12.784182Z","iopub.execute_input":"2024-12-20T05:38:12.784683Z","iopub.status.idle":"2024-12-20T05:42:59.562447Z","shell.execute_reply.started":"2024-12-20T05:38:12.784655Z","shell.execute_reply":"2024-12-20T05:42:59.561642Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.03k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec184a7ca2d841f88c8066ab58f19a21"}},"metadata":{}},{"name":"stderr","text":"`low_cpu_mem_usage` was None, now default to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/62.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a8b8b422c0e4de6aa1615bcc70a8af0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"feadee482d704354ad1060df071ef3b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47ee328ff3674e60bc5e5c28186ef789"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c26fad7efc3476caa9a91258c0dc4e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/1.74G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"796d36df461c43d398c1c1bc878b4c11"}},"metadata":{}},{"name":"stderr","text":"`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\nGemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n`config.hidden_activation` if you want to override this behaviour.\nSee https://github.com/huggingface/transformers/pull/29402 for more details.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0cf8ba2caf2461b8fc6ee3806e6a804"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7c1eefca77a43568fc38dc3470bba95"}},"metadata":{}}],"execution_count":10},{"id":"7654cd6e-b62d-4273-b03d-60c61c0d7e40","cell_type":"code","source":"model.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T05:42:59.563707Z","iopub.execute_input":"2024-12-20T05:42:59.564224Z","iopub.status.idle":"2024-12-20T05:42:59.574985Z","shell.execute_reply.started":"2024-12-20T05:42:59.564184Z","shell.execute_reply":"2024-12-20T05:42:59.574024Z"}},"outputs":[{"name":"stdout","text":"trainable params: 5,649,408 || all params: 2,929,115,888 || trainable%: 0.1929\n","output_type":"stream"}],"execution_count":11},{"id":"bb1f970e-503c-4571-bfb4-5cde62567fc9","cell_type":"markdown","source":"## Setup Training Configurations","metadata":{}},{"id":"d7debc62-d578-410d-b205-e1fb6b657b7b","cell_type":"code","source":"config = config = {\n            \"max_epochs\": 100,\n\n          \"val_check_interval\": 300, \n\n          \"gradient_clip_val\": 1.0,\n\n          \"accumulate_grad_batches\": 8,\n\n          \"lr\": 3e-4,\n\n          \"batch_size\": 3,\n\n          \"seed\":1234,\n\n          \"num_nodes\": 1,\n\n          \"warmup_steps\": 50,\n\n          \"result_path\": \"./result\",\n\n          \"verbose\": True,\n\n}\n\n\n\nmodel_module = PaliGemma_Finetuned_Model(config, model, processor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T05:42:59.576172Z","iopub.execute_input":"2024-12-20T05:42:59.576912Z","iopub.status.idle":"2024-12-20T05:42:59.586934Z","shell.execute_reply.started":"2024-12-20T05:42:59.576856Z","shell.execute_reply":"2024-12-20T05:42:59.586230Z"}},"outputs":[],"execution_count":12},{"id":"941ae29e-765d-4500-b384-39c8e332572a","cell_type":"markdown","source":"## Define Callbacks","metadata":{}},{"id":"59037b17-be93-45c6-8592-1d37581bb2e4","cell_type":"code","source":"from lightning.pytorch.callbacks import Callback\n\nfrom lightning.pytorch.callbacks.early_stopping import EarlyStopping\n\nfrom huggingface_hub import HfApi\n\n\n\napi = HfApi()\n\n\n\nFINETUNED_MODEL_ID=\"ball1433/PaliGemma-StanfordCars-finetuned\"\n\n\n\nclass Print_TrainValidation_ResultCallback(Callback):\n\n    def on_train_epoch_end(self, trainer, pl_module):\n\n        # print the average of training loss \n\n        print(f'Average Training Loss: {np.mean(pl_module.train_losses)}')\n\n\n\n        # print the average of edit distance score\n\n        print(f'Average Validation Score: {np.mean(pl_module.val_scores)}')\n\n\n\n        # reset the list\n\n        pl_module.train_losses = []\n\n        pl_module.val_scores = []\n\n\n\n\n\nclass PushToHubCallback(Callback):\n\n    def on_train_epoch_end(self, trainer, pl_module):\n\n        print(f\"Pushing model to the hub, epoch {trainer.current_epoch}\")\n\n        pl_module.model.push_to_hub(FINETUNED_MODEL_ID,\n\n                                    commit_message=f\"Training in progress, epoch {trainer.current_epoch}\")\n\n\n\n    def on_train_end(self, trainer, pl_module):\n\n        print(f\"Pushing model to the hub after training\")\n\n        pl_module.processor.push_to_hub(FINETUNED_MODEL_ID,\n\n                                    commit_message=f\"Training done\")\n\n        pl_module.model.push_to_hub(FINETUNED_MODEL_ID,\n\n                                    commit_message=f\"Training done\")\n\n\n\nearly_stop_callback = EarlyStopping(monitor=\"val_edit_distance\", patience=20, verbose=False, mode=\"min\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T05:42:59.589146Z","iopub.execute_input":"2024-12-20T05:42:59.589421Z","iopub.status.idle":"2024-12-20T05:42:59.603225Z","shell.execute_reply.started":"2024-12-20T05:42:59.589393Z","shell.execute_reply":"2024-12-20T05:42:59.602308Z"}},"outputs":[],"execution_count":13},{"id":"95a43f57-6379-4bf5-95b8-745eb9ec1737","cell_type":"markdown","source":"## Training","metadata":{}},{"id":"c5b43acc-eecd-483c-bb86-607deab32d7f","cell_type":"code","source":"# define trainer \n\ntrainer = L.Trainer(\n\n        devices=-1, \n\n        accelerator=\"auto\",\n\n        max_epochs=config.get(\"max_epochs\"),\n\n        accumulate_grad_batches=config.get(\"accumulate_grad_batches\"),\n\n        val_check_interval=config.get(\"val_check_interval\"),\n\n        gradient_clip_val=config.get(\"gradient_clip_val\"),\n\n        precision=\"16-mixed\",\n\n        limit_val_batches=1.0,\n\n        num_sanity_val_steps=5,\n\n        callbacks=[PushToHubCallback(), Print_TrainValidation_ResultCallback(), early_stop_callback],\n\n)\n\n\n\ntrainer.fit(model_module)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T05:42:59.604333Z","iopub.execute_input":"2024-12-20T05:42:59.605166Z","iopub.status.idle":"2024-12-20T06:23:52.968510Z","shell.execute_reply.started":"2024-12-20T05:42:59.605127Z","shell.execute_reply":"2024-12-20T06:23:52.966814Z"}},"outputs":[{"name":"stderr","text":"INFO: Using 16bit Automatic Mixed Precision (AMP)\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: HPU available: False, using: 0 HPUs\nINFO: `Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name  | Type                 | Params | Mode \n-------------------------------------------------------\n0 | model | PeftModelForCausalLM | 1.7 B  | train\n-------------------------------------------------------\n5.6 M     Trainable params\n1.7 B     Non-trainable params\n1.7 B     Total params\n6,925.987 Total estimated model params size (MB)\n2072      Modules in train mode\n593       Modules in eval mode\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","output_type":"stream"},{"name":"stdout","text":"SANITY CHECKING STEP\nbatch_decoded input_ids: ['What is the model of a car in the image?\\n', 'What is the model of a car in the image?\\n', 'What is the model of a car in the image?\\n']\nlabels: ['AM General Hummer SUV 2000<eos>', 'AM General Hummer SUV 2000<eos>', 'AM General Hummer SUV 2000<eos>']\nprediction: ['mer h1', 'mer h1', 'mer h1']\nedit_distance_score: 0.8709677419354839\nSANITY CHECKING STEP\nbatch_decoded input_ids: ['What is the model of a car in the image?\\n', 'What is the model of a car in the image?\\n', 'What is the model of a car in the image?\\n']\nlabels: ['AM General Hummer SUV 2000<eos>', 'AM General Hummer SUV 2000<eos>', 'AM General Hummer SUV 2000<eos>']\nprediction: ['mer h2', 'mer h1', 'mer h1']\nedit_distance_score: 0.8602150537634409\nSANITY CHECKING STEP\nbatch_decoded input_ids: ['What is the model of a car in the image?\\n', 'What is the model of a car in the image?\\n', 'What is the model of a car in the image?\\n']\nlabels: ['AM General Hummer SUV 2000<eos>', 'AM General Hummer SUV 2000<eos>', 'AM General Hummer SUV 2000<eos>']\nprediction: ['mer h1', 'mer h2', 'mer h1']\nedit_distance_score: 0.8602150537634409\nSANITY CHECKING STEP\nbatch_decoded input_ids: ['What is the model of a car in the image?\\n', 'What is the model of a car in the image?\\n', 'What is the model of a car in the image?\\n']\nlabels: ['AM General Hummer SUV 2000<eos>', 'AM General Hummer SUV 2000<eos>', 'AM General Hummer SUV 2000<eos>']\nprediction: ['mer h1', 'mer h2', 'mer h1']\nedit_distance_score: 0.8602150537634409\nSANITY CHECKING STEP\nbatch_decoded input_ids: ['What is the model of a car in the image?\\n', 'What is the model of a car in the image?\\n', 'What is the model of a car in the image?\\n']\nlabels: ['AM General Hummer SUV 2000<eos>', 'AM General Hummer SUV 2000<eos>', 'AM General Hummer SUV 2000<eos>']\nprediction: ['mer h1', 'mer h1', 'mer h1']\nedit_distance_score: 0.8709677419354839\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64648badba78438bb8c6f15d99361fee"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e72857a07b164ad7ace5e0850e12b29b"}},"metadata":{}},{"name":"stdout","text":"VALIDATION STEP\nbatch_decoded input_ids: ['What is the model of a car in the image?\\n', 'What is the model of a car in the image?\\n', 'What is the model of a car in the image?\\n']\nlabels: ['AM General Hummer SUV 2000<eos>', 'AM General Hummer SUV 2000<eos>', 'AM General Hummer SUV 2000<eos>']\nprediction: ['MER H2 SUT Crew Cab 2007', 'MER H1 Crew Cab 2000', 'MER H1 Crew Cab 2000']\nedit_distance_score: 0.7311827956989246\n","output_type":"stream"},{"name":"stderr","text":"INFO: \nDetected KeyboardInterrupt, attempting graceful shutdown ...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py:141\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(data_fetcher)\n\u001b[0;32m--> 141\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_advance_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py:295\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.on_advance_end\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    293\u001b[0m     call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_validation_model_zero_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 295\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py:178\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py:135\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py:396\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    391\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    395\u001b[0m )\n\u001b[0;32m--> 396\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:319\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 319\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:411\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[9], line 71\u001b[0m, in \u001b[0;36mPaliGemma_Finetuned_Model.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     67\u001b[0m input_ids, attention_mask, pixel_values, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m---> 71\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mbatch_decode(generated_ids[:, input_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:1838\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1837\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1838\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1839\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:3206\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3205\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3206\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3208\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/paligemma/modeling_paligemma.py:522\u001b[0m, in \u001b[0;36mPaliGemmaForConditionalGeneration.forward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep)\u001b[0m\n\u001b[1;32m    518\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_causal_mask(\n\u001b[1;32m    519\u001b[0m     attention_mask, token_type_ids, inputs_embeds, past_key_values, cache_position, is_training\n\u001b[1;32m    520\u001b[0m )\n\u001b[0;32m--> 522\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py:1072\u001b[0m, in \u001b[0;36mGemmaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1072\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1085\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py:828\u001b[0m, in \u001b[0;36mGemmaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 828\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    838\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py:553\u001b[0m, in \u001b[0;36mGemmaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_forward\u001b[39m(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 165\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hf_hook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mno_grad:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:364\u001b[0m, in \u001b[0;36mAlignDevicesHook.pre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m         set_module_tensor_to_device(\n\u001b[1;32m    356\u001b[0m             module,\n\u001b[1;32m    357\u001b[0m             name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m             tied_params_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map,\n\u001b[1;32m    362\u001b[0m         )\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device), \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mskip_keys\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:178\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m honor_type(\n\u001b[1;32m    176\u001b[0m         tensor, (send_to_device(t, device, non_blocking\u001b[38;5;241m=\u001b[39mnon_blocking, skip_keys\u001b[38;5;241m=\u001b[39mskip_keys) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tensor)\n\u001b[1;32m    177\u001b[0m     )\n\u001b[0;32m--> 178\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMapping\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(skip_keys, \u001b[38;5;28mstr\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/typing.py:994\u001b[0m, in \u001b[0;36m_BaseGenericAlias.__instancecheck__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__instancecheck__\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj):\n\u001b[0;32m--> 994\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__subclasscheck__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/typing.py:1154\u001b[0m, in \u001b[0;36m_SpecialGenericAlias.__subclasscheck__\u001b[0;34m(self, cls)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtyping.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name\n\u001b[0;32m-> 1154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__subclasscheck__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mcls\u001b[39m):\n\u001b[1;32m   1155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mcls\u001b[39m, _SpecialGenericAlias):\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 29\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# define trainer \u001b[39;00m\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m         devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m )\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_module\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n","\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"],"ename":"NameError","evalue":"name 'exit' is not defined","output_type":"error"}],"execution_count":14},{"id":"ac92c16d-1ace-4e15-94f4-67c70a50d3a5","cell_type":"markdown","source":"## Inference","metadata":{}}]}